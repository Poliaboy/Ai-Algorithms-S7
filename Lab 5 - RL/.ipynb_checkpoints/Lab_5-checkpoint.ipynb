{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert-block alert-info'>\n",
    "    <br>\n",
    "    <h1 align=\"center\"><b>  Lab session 5 :</b> Reinforcement Learning </h1>\n",
    "    <h3 align=\"center\">Artificial Intelligence Algorithms </h3>\n",
    "    <h5 align=\"center\">MESIIN476023</a></h5>\n",
    "    <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color='darkblue'>**Submission**</front>\n",
    "\n",
    "- Complete assignment 1 and assignment 2.\n",
    "- Submit your completed notebook to the designated area: section Lab sessions –> Lab5 –> Deposit DIA *) on De Vinci Learning.\n",
    "\n",
    "- You may collaborate with one other classmate on this assignment. Please include both names at the top of the notebook or in the file name for clarity.\n",
    "\n",
    "- The deadline is before December 22, 2023.\n",
    "\n",
    "- Take the time to understand the two approaches presented in this practical work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "## Lava environments\n",
    "The environments used are LavaFloor (visible in the figure) and its variations.\n",
    "\n",
    "![Lava](images/lava.png)\n",
    "\n",
    "The agent starts in cell $(0, 0)$ and has to reach the treasure in $(2, 3)$. There are walls in cell  $(1, 1)$, the floor is covered with lava, and there is a black pit of death.\n",
    "\n",
    "\n",
    "We assume that the agent can't comfortably perform its actions that instead have a stochastic outcome (visible in the figure):\n",
    "\n",
    "![Dynact](images/dynact.png)\n",
    "\n",
    "The action dynamics is the following:\n",
    "- $P(0.8)$ of moving in the desired direction\n",
    "- $P(0.1)$ of moving in a direction 90° with respect to the desired direction\n",
    "\n",
    "Finally, since the floor is covered in lava, the agent receives a negative reward for each of its steps!\n",
    "\n",
    "- -0.04 for each lava cell (L)\n",
    "- -5 for the black pit (P). \n",
    "- +1 for the treasure (G). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "\n",
    "\n",
    "import gym_lab5, envs_lab5\n",
    "import numpy as np\n",
    "from utils_lab5.ai_lab_functions import *\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Properties \n",
    "\n",
    "In addition to the varables of the environments you have been using in the previous sessions, there are also a few more:\n",
    "\n",
    "- $T$: matrix of the transition function $T(s, a, s') \\rightarrow [0, 1]$\n",
    "- $RS$: matrix of the reward function $R(s) \\rightarrow \\mathbb{R}$\n",
    "\n",
    "The available actions are still Left, Right, Up, Down.\n",
    "\n",
    "#### Code Hints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of actions:  4\n",
      "Actions:  {0: 'L', 1: 'R', 2: 'U', 3: 'D'}\n",
      "Reward:\n",
      " [[-0.04 -0.04 -0.04 -0.04]\n",
      " [-0.04  0.   -0.04 -5.  ]\n",
      " [-0.04 -0.04 -0.04  1.  ]]\n",
      "Reward of goal state: 1.0\n",
      "Probability from (0, 0) to (0, 1) with action right: 0.8\n",
      "Probability from (0, 0) to (2, 3) with action left: 0.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym_lab5.make(\"LavaFloor-v0\") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "current_state = env.pos_to_state(0, 0)\n",
    "next_state = env.pos_to_state(0, 1)\n",
    "goal_state = env.pos_to_state(2, 3)\n",
    "\n",
    "print(\"Number of actions: \", env.action_space.n)\n",
    "print(\"Actions: \", env.actions)\n",
    "print(\"Reward:\\n\", np.asarray(env.RS).reshape(env.rows, env.cols))\n",
    "print(\"Reward of goal state:\", env.RS[goal_state])\n",
    "print(\"Probability from (0, 0) to (0, 1) with action right:\", env.T[current_state, 1, next_state])\n",
    "print(\"Probability from (0, 0) to (2, 3) with action left:\", env.T[current_state, 1, goal_state])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dispaly the environment transition matrix, and take the time to understand the output\n",
    "\n",
    "# your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='darkred'>Assignment 1: Value Iteration Algorithm</front>\n",
    "\n",
    "Your first assignment is to implement the Value Iteration algorithm on LavaFloor. The solution returned by your algorithm must be a 1-d array of action identifiers where the $i$-th action refers to the $i$-th state.  You can perform all the test on different versions of the environment, but with the same structure:  *NiceLavaFloor*, *VeryBadLavaFloor*, and *HugeLavaFloor*.\n",
    "\n",
    "<img src=\"images/value-iteration.png\" width=\"600\">\n",
    "\n",
    "You must implement the *value_iteration* function. Notice that the value iteration approach return a matrix with the value for each state, the function *values_to_policy* automatically convert this matrix in the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(problem, maxiters=50, discount=0.9, max_error=1e-3):\n",
    "    \"\"\"\n",
    "    Performs the value iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        problem: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        max_error: the maximum error allowd in the utility of any state\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code\n",
    "            \n",
    "    return values_to_policy(np.asarray(U), env) # automatically convert the value matrix U to a policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code executes the Value Iteration algorithm and prints the resulting policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envname = \"LavaFloor-v0\"\n",
    "#envname = \"NiceLavaFloor-v0\"\n",
    "#envname = \"VeryBadLavaFloor-v0\"\n",
    "#envname = \"HugeLavaFloor-v0\"\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------------------------------------------\")\n",
    "print(\"\\tEnvironment: {} \\n\\tValue Iteration\".format(envname))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "env = gym_lab5.make(envname)\n",
    "print(\"\\nRENDER:\")\n",
    "env.render()\n",
    "\n",
    "t = timer()\n",
    "policy = value_iteration(env)\n",
    "\n",
    "print(\"\\nTIME: \\n{}\".format(round(timer() - t, 4)))\n",
    "print(\"\\nPOLICY:\")\n",
    "print(np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct results can be found [here](lesson_3_results.txt).\n",
    "\n",
    "### <font color='darkred'>Assignment 2: Policy Iteration Algorithm</front>\n",
    "\n",
    "Your second assignment is to implement the Policy Iteration algorithm on LavaFloor. The solution returned by your algorithm must be a 1-d array of action identifiers where the $i$-th action refers to the $i$-th state. You can perform all the test on different versions of the environment, but with the same structure: *HugeLavaFloor*, *NiceLavaFloor* and *VeryBadLavaFloor*.\n",
    "\n",
    "<img src=\"images/policy-iteration.png\" width=\"600\">\n",
    "\n",
    "For the *policy evaluation step*, implement this function repeating the update for an arbitrary number of steps (e.g., 10):\n",
    "\n",
    "<img src=\"images/policy-evaluating.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(problem, maxiters=50, discount=0.9, max_error=1e-3):\n",
    "    \"\"\"\n",
    "    Performs the policy iteration algorithm for a specific environment\n",
    "    \n",
    "    Args:\n",
    "        problem: OpenAI Gym environment\n",
    "        maxiters: timeout for the iterations\n",
    "        discount: gamma value, the discount factor for the Bellman equation\n",
    "        \n",
    "    Returns:\n",
    "        policy: 1-d dimensional array of action identifiers where index `i` corresponds to state id `i`\n",
    "    \"\"\"\n",
    "    \n",
    "    # Your code\n",
    "    \n",
    "    return np.asarray(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code executes the Value Iteration algorithm and prints the resulting policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envname = \"LavaFloor-v0\"\n",
    "#envname = \"VeryBadLavaFloor-v0\"\n",
    "#envname = \"NiceLavaFloor-v0\"\n",
    "#envname = \"HugeLavaFloor-v0\"\n",
    "\n",
    "\n",
    "print(\"\\n----------------------------------------------------------------\")\n",
    "print(\"\\tEnvironment: {} \\n\\tPolicy Iteration\".format(envname))\n",
    "print(\"----------------------------------------------------------------\")\n",
    "\n",
    "env = gym_lab5.make(envname)\n",
    "print(\"\\nRENDER:\") # a rendering of the environment's current state.\n",
    "env.render()\n",
    "\n",
    "t = timer()\n",
    "policy = policy_iteration(env)\n",
    "\n",
    "print(\"\\nTIME: \\n{}\".format(round(timer() - t, 4)))\n",
    "print(\"\\nPOLICY:\")\n",
    "print(np.vectorize(env.actions.get)(policy.reshape(env.rows, env.cols)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct results can be found [here](lesson_3_results.txt).\n",
    "\n",
    "### Comparison\n",
    "\n",
    "The following code performs a comparison between Value Iteration and Policy Iteration, by plotting the accumulated rewards of each episode with iterations in range $[1, 50]$ (might take a long time on big environments). You can perform all the test on a different versions of the environment, but with the same structure: *HugeLavaFloor*.\n",
    "\n",
    "The function **run_episode(envirnonment, policy, max_iteration)** run an episode on the given environment using the input policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envname = \"LavaFloor-v0\"\n",
    "#envname = \"HugeLavaFloor-v0\"\n",
    "maxiters = 50\n",
    "\n",
    "env = gym_lab5.make(envname)\n",
    "\n",
    "series = []  # Series of learning rates to plot\n",
    "liters = np.arange(maxiters + 1)  # Learning iteration values\n",
    "liters[0] = 1\n",
    "elimit = 100  # Limit of steps per episode\n",
    "rep = 10  # Number of repetitions per iteration value\n",
    "virewards = np.zeros(len(liters))  # Rewards array\n",
    "c = 0\n",
    "\n",
    "t = timer()\n",
    "\n",
    "# Value iteration\n",
    "for i in tqdm(liters, desc=\"Value Iteration\", leave=True):\n",
    "    reprew = 0\n",
    "    policy = value_iteration(env, maxiters=i)  # Compute policy\n",
    "    # Repeat multiple times and compute mean reward\n",
    "    for _ in range(rep):\n",
    "        reprew += run_episode(env, policy, elimit)  # Execute policy\n",
    "    virewards[c] = reprew / rep\n",
    "    c += 1\n",
    "series.append({\"x\": liters, \"y\": virewards, \"ls\": \"-\", \"label\": \"Value Iteration\"})\n",
    "\n",
    "vmaxiters = 5  # Max number of iterations to perform while evaluating a policy\n",
    "pirewards = np.zeros(len(liters))  # Rewards array\n",
    "c = 0\n",
    "\n",
    "# Policy iteration\n",
    "for i in tqdm(liters, desc=\"Policy Iteration\", leave=True):\n",
    "    reprew = 0\n",
    "    policy = policy_iteration(env, maxiters=i)  # Compute policy\n",
    "    # Repeat multiple times and compute mean reward\n",
    "    for _ in range(rep):\n",
    "        reprew += run_episode(env, policy, elimit)  # Execute policy\n",
    "    pirewards[c] = reprew / rep\n",
    "    c += 1\n",
    "series.append({\"x\": liters, \"y\": pirewards, \"ls\": \"-\", \"label\": \"Policy Iteration\"})\n",
    "\n",
    "print(\"Execution time: {0}s\".format(round(timer() - t, 4)))\n",
    "np.set_printoptions(linewidth=10000)\n",
    "\n",
    "plot(series, \"Learning Rate\", \"Iterations\", \"Reward\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correct results for comparison can be found here below. Notice that since the executions are stochastic the charts could differ: the important thing is the global trend and the final convergence to an optimal solution.\n",
    "\n",
    "**Standard Lava Floore results comparison**\n",
    "<img src=\"images/results-standard.png\" width=\"600\">\n",
    "\n",
    "**Huge Lava Floore results comparison**\n",
    "<img src=\"images/results-huge.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
